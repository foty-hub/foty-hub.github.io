---
layout: post
title: "WTF is MPC?"
subtitle: "A worked introduction to Model-Predictive Control for Machine Learners."
math: true
draft: true
---

What is MPC - how does it relate to RL?

If you're an AI researcher who is not fully bought into the LLM craze, you might have heard Yann LeCun argue that AI researchers should reject Reinforcement Learning in favour of Model Predictive Control. But what does that actually mean? In this post, I'll briefly describe MPC - assuming some background with RL/ML more broadly. Most of the 

This ties into the excitement around world models. If we can train models that predict how the environment unfolds, MPC is an approach that takes advantage of those predictions to perform action selection. In this post, I'll describe the principles behind MPC, assuming some familiarity with RL/ML. I'll skip over some of the more involved maths; interested readers should check out Bertsekas's book Reinforcement Learning & Optimal Control. If you prefer reading code, here's a companion Colab notebook with a NumPy implementation.

TBD - why is this exciting to LeCun? This is a simple solution to action selection without having to learn via RL. And (presumably) it's signifcantly less data inefficient than RL - where you've got the classic issue that one episode confers very little information - just as much as you get some reward.

TO COME - gif of an MPC controller rolling out -> incrementing step by step


# Out of Control

To understand the control problem, imagine you've got a drone. You can control the rotor speed with a knob called $u$, and you want your drone to sit at a fixed reference height $r$. We'll do that by changing the underyling state $x$, which plays the same role as the RL state $s$. Like in RL, the state is *Markovian* -- it contains all the necessary information to infer future states. In this case, the state contains the drone's vertical position and velocity $x=[p, \hspace{0.2em} \dot{p}]$. Since we can't actually know $x$ precisely, we estimate it via measurements $y$. Our goal is to minimise the error $\|y-r\|.$ With that set up, we can define a basic model of how the drone works

$$
\begin{aligned}
x_{t+1} &= Ax_t + Bu_t\\
y_t &= Cx_t + Du_t.\\
\end{aligned}
$$

The matrices $A,B,C,D$ define our model of the system, dictating how things change from time-step to time-step:
- $A$ tells you how $x_t$ transitions to $x_{t+1}$. These are just the laws of physics - given a particular position and velocity, what state would the drone be in on the next time step?
- $B$ tells you how your control signal $u$ affects the state. If the drones motors are weak, $B$ will be small - you need a big $u$ to get a little speed.
- $C$ is an emission matrix - it tells you what $y$ you'll measure based on the underlying state $x$.
- $D$ tells you whether tweaking the control signal will also directly affect the measurements. $D=0$ in our example, since inertia is a thing, you can't instantly change position by spinning your rotors faster. But if we were measuring the instantaneous acceleration instead of position, then $u$ would have an immediate impact and $D$ would be nonzero.

These are pretty simplified. Common extensions could include adding noise terms, augmenting the state space ($\tilde{x}=[x, d]$) to manage disturbances, or making the system nonlinear by collapsing the nice matrices into nasty functions $x_{t+1} = f(x_t, u_t).$

<figure class="illustration">
  <img src="/assets/images/mpc_illustration.svg" width="560" height="150" alt="MPC illustration">
  <figcaption>Illustration of a simple MPC model. Everything here is deterministic, so we can unroll it to predict a trajectory.</figcaption>
</figure>

# Is This Loss?

The keen-eyed reader will notice that everything in our model is deterministic. In other words, if we know the starting state $x$, we can predict the future for any sequence of control values $u_t$. How convenient! This is the basic concept underpinning MPC - at each timestep we'll unroll our model to predict the future, and choose the values of $u$ that give rise to the best predicted trajectory. But we can't choose the "best" trajectory without some kind of scoring. For that we'll use a cost function

$$
\begin{aligned}
 J &= \sum_{\tau=t}^{t+H} \bigl [ (y_\tau - r_\tau)^T Q (y_\tau - r_\tau) + (u_{\tau} - u_{\tau - 1})^T S (u_{\tau} - u_{\tau - 1}) \bigr ] \\
&=\bigl [ \underbrace{(Y_t - R_t)^T \bar{Q} (Y_t - R_t)}_\text{Error term} + \underbrace{(\Delta U_t)^T \bar{S} (\Delta U_t)}_\text{Regularisation} \bigr ].
\end{aligned}
$$

The first "error term" in our MPC cost function defines how strongly an error $\|y_t - r_t\|$ is penalised, and the second term disincentivises rapid changes in the control signal (to prevent your controller instantly slamming the rotors to infinity). We sum those two terms over every step in the rollout to score a trajectory. The second form is just vectorised, by stacking variables like $Y_t = [y_t, y_{t+1}, \dots, y_{t+H}]$, and so on. You might notice there's a direct correspondence between the MPC objective and regularised least squares

$$
\min_w \bigl [ (Y - Xw)^T (Y - Xw) + \lambda w^T w \bigr].
$$

The only real difference is the use of the matrices $\bar{Q}$ and $\bar{S}$ in the inner products, but these are just more complex weighting factors that let you prioritise certain dimensions of the error. In ML, we let the neural net figure that out. 

So both Control and ML use loss functions (or *cost* functions). In ML we usually stop there - we define a loss function, set up an algorithm to minimise it (eg. backprop, ES, TD($\lambda$)) and let it rip. In Control, the cost function is just the start. Because you have a mathematically defined system of equations, you can solve them directly rather than relying on gradient descent to bodge a solution.

# Predictive What?

At least, you *could* solve the equations directly. It's a bit tedious though, so I'll skip over most of it. The trick is realising that you can unroll the measurements as follows:
<details class="maths-warning" markdown="block">
<summary><strong>Maths warning:</strong> detailed derivation (optional)</summary>

$$
Y_t = \Lambda x_{t} + \Phi U_t \\
$$

where $\Lambda, \Phi$ are matrices encoding the repated application of the update matrices

$$
\begin{aligned}
\Lambda &= \begin{bmatrix} 
C \\ 
CA \\ 
CA^2 \\ 
\vdots \\ 
CA^{N-1} 
\end{bmatrix}, \quad
\Phi = \begin{bmatrix} 
D & & & & \\ 
CB & D & & & \\ 
CAB & CB & D & & \\ 
\vdots & & & \ddots & \\ 
CA^{N-2}B & \dots & \dots & CB & D 
\end{bmatrix}.
\end{aligned}
$$

Note that $\Phi$ has the same block-causal structure as an attention mask in a transformer. This is autoregressive, but we can produce the whole trajectory in one go because our model is linear. These equations are just saying that the trajectory of measurements is a function of our initial state, $x_t$, and our sequence of control values $U_t$.

TKTKTKT - the next step, how do we unfurl everything to define $H, f$?

</details>

Continuing down the garden path leads to the following cost function

$$
\boxed{J(U_t) = \frac{1}{2} U_t^T H U_t + U_t^T f}
$$

$H$ and $f$ are composed of the matrices we've defined previously. This cost function is appealing because it's in *quadratic form* -- which means it can be solved with the following guarantees: we'll get the global optimum, we'll converge quickly, and we can solve with constraints, like demanding that $0 < u < 10{,}000$.

# Example solution




*DISCLAIMER: I'm only learning bits and bobs about Control on the side, and am by no means an expert. I've kept everything in this post fairly simple to tkTKTKT*
