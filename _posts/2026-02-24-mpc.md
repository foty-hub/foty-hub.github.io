---
layout: post
title: "WTF is MPC?"
subtitle: "An intro to Model-Predictive Control for Machine Learners."
math: true
draft: false
---

What is MPC - how does it relate to RL?

If you're an AI researcher who is not fully bought into the LLM craze, you might have heard (washed up hasbeen\|contrarian genius) - delete as appropriate - Yann LeCun's argue that AI researchers should reject Reinforcement Learning in favour of Model Predictive Control. But wait, what is Model-Predictive Control? In this post, I'll briefly describe MPC - assuming some background with RL/ML more broadly.

# Out of Control

Basically, you have a system, with a knob. You can tweak a knob, called $u$, to try and get your system to do what you want. An example might be twiddling with the rotor speeds of a drone to get it to a specific height. Mathematically, we want a measurement $y$ (our measurement of the current height) to match a target value $r$ (the height we're aiming for), and we'll do that by changing the underlying physical state $x$. Note that $x$ can contain multiple different kinds of coordinate - in the drone example, it can contain both the position and vertical velocity $x=[p, \frac{dp}{dt}].$

Control and ML/RL both use a loss function (or *cost* function). The big difference is that ML/RL usually stops at the loss - we define it, set up an algorithm to minimise it (eg. backprop, ES, TD($\lambda$)) and let it rip. In Control, the cost function is just the start - because you have a mathematically defined system of equations, you can derive underlying equations and solve them mathematically, rather than relying on gradient descent to bodge a solution.

# The Next Top Model

So what does a Control model look like? Here are some simple equations for linear MPC

$$
\begin{aligned}
x_{t+1} &= Ax_t + Bu_t\\
y_t &= Cx_t + Du_t.\\
\end{aligned}
$$

As a reminder, $x$ is the true state (position and speed) and $y$ contains observations (eg. current height). The matrices $A,B,C,D$ define how we update from step-to-step:

- $A$ tells you how $x_t$ transitions to $x_{t+1}$ with no additional input. For the drone, these are just the laws of physics - given a particular position and velocity, where will the drone be on the next time step and with what speed?
- $B$ tells you how your control signal $u$ affects the state. If the drones motors are weak, $B$ will be small - you need a lot of $u$ to get a little bit of speed.
- $C$ is an emission matrix - it tells you what $y$ you'll measure based on the underlying state $x$.
- $D$ tells you whether tweaking the control signal will also directly affect the measurements. In our drone example, $D$ would be 0, since you can't instantly change the position by spinning your rotors faster. But if we were measuring the acceleration, then you can imagine that $u$ would have a more immediate impact and $D$ would be nonzero.

This is really a simplest possible scenario. You'd typically also have some noise terms. And because our system is linear, we can represent all the updates as matrices, rather than nonlinear functions like $x=f(x_t, u_t)$.

# Predictive What?

So we have a model of our system - now we want to ask what we can do with it. Well, the idea of MPC is simple: at each time step, use your model to predict into the future, and compute the optimal rollout over some horizon of $H$ time steps. Because we're dealing with a linear system, we can actually derive some simple terms. For example, here's the loss function that we'll minimise for our rollout

$$
\min _u J = \min_u \bigl [ \underbrace{(Y_t - R_t)^T Q (Y_t - R_t)}_\text{Error term} + \underbrace{(\Delta u)^T S (\Delta u)}_\text{Regularisation} \bigr ].
$$

If you squint, this looks extremely reminiscent of a classic least squares error with L2 regularisation

$$
\min_\theta L = \min_\theta \bigl [ (Y - Xw)^T (Y - Xw) + \lambda w^T w].
$$

The two ideas are very closely related - in MPC, we're defining a cost function in two parts. The first "error term" defines how strongly you penalise a distance between the desired value ($R_t$) and the output value ($Y_t$). The second term behaves like a regulariser in ML, it penalises rapid changes in the control signal .


*DISCLAIMER: I'm only learning bits and bobs about Control on the side, and am by no means an expert. I've kept everything in this post fairly simple to tkTKTKT*