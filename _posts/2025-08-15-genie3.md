---
layout: post
title: "Notes on Genie 3"
---

Deepmind's Genie 3 came out a few weeks ago and set various corners of the internet alight. Those paying attention will note the incredible gain in capabilities since Genie 1 a year ago. Deepmind has scaled from 5 second blurry snippets of 2D platformers, to minute-long, immersive, high res, interactions with consistent worlds. The model runs in realtime - although Deepmind didn't disclose the compute per instance.

To show off the model, they released numerous impressive demos, with more shared on Twitter in the following weeks. Dragonflying, jetskiing and even an eerie video with a world within a world. But it's worth remembering that demos like these are cherrypicked, intended to highlight strengths and paper over limitations. I wanted to note down a few of the things I *didn't see* in the Genie 3 demos; not as a nitpick or complaint, but for research challenges that I think will be interesting and relevant to embodied AI.
## Static Environments
Most environments shown in the videos are relatively static. That's not to say there's no physics: we see water splashing, dust clouds, hard body physics and mirrored reflections. But Genie is predominantly trained on games, and those are the game physics. When the model strays away from gameplay things can make less sense. That's corroborated by Tejas Kulkarni on Twitter; behind the curtains, physics simulation is still limited.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Special thanks to <a href="https://twitter.com/GoogleDeepMind?ref_src=twsrc%5Etfw">@GoogleDeepMind</a> for inviting me to try out Genie 3. I&#39;m excited to share my thoughts on this early research prototype and also some of my live recordings below:<br><br>I spent the whole day playing with the system and when it works, it is truly mind blowingðŸ¤¯. It isâ€¦ <a href="https://t.co/JPW5sPEeF5">pic.twitter.com/JPW5sPEeF5</a></p>&mdash; Tejas Kulkarni (@tejasdkulkarni) <a href="https://twitter.com/tejasdkulkarni/status/1952737669894574264?ref_src=twsrc%5Etfw">August 5, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

I think that while games are the majority of the pretraining corpus this will be a challenge; devs have to choose what can and can't have interactivity, and some interactions like cloth-on-cloth, feathers and fur can't be rendered by games in realtime. In a related note, other agents are rare unless requested in a world event, likely because they pose a similarly difficult challenge.

<video width="560" height="315" controls>
  <source src="/videos/jetski.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
The boat collision at 0:25 is clunky, in contrast to realistic interactions at 0:05

<video width="560" height="315" controls>
  <source src="/videos/leaves_static.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
The agent doesn't collide the leaves, even when walking through them at 0:39.
## World interaction
With some exceptions, like the mindblowing painter demo, the videos only show navigation. Agents traverse a world, revealing the surroundings, but they rarely interact with the world.

To be clear, this is a hard problem. Total interactivity means virtually unbounded branching in agent interactions and world states that the model has to be able to handle. And again, with a game-heavy corpus, interactivity in the training data will be limited. I'd bet Genie is capable of creating an FPS, but a "set-the-bed" simulator would prove much more challenging. Until a Genie-like system is deployed in the real world and allowed to interact and learn for itself, I'm skeptical that we'll see truly free interaction purely from pretraining.

This is most clear in the SIMA demos. The goals involves an agent walking to something, like a hose, but it can't pick that hose up and water the garden.

## Action consistency & Steerability
The inconsistency in actions manifests in two ways: sometimes actions shown on screen don't appear to affect the video gen, and sometimes the meaning of an actions changes over time.

<video width="560" height="315" controls>
  <source src="/videos/critter.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
At 0:22 the agent jumps before the button is pressed
<video width="560" height="315" controls>
  <source src="/videos/cat_reverse.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
The forward action is inconsistent - early it pivots the cat and aims forwards, but at the end it moves backwards without redirecting the cat.

These examples make me wonder how much conflict there is between visual fidelity and steerability. The critter jumping over the gap or the firefly moving forwards is likely very consistent with the training data. But that kind of consistency gets in the way of steerability; if the agent is meant to fall down the hole, that should happen. Poor steerability could cause issues for agent learning in world models. If they get too comfortable with a world which adjusts for their mistakes, there's no need to properly learn how to handle the environment.

## Conclusion
To reiterate, the Genie models are incredible, and they've kickstarted an energetic world of research into videogen world models. Diffusion models have already been [deployed in robotics](https://arxiv.org/pdf/2302.11550) to exciting results. Like Wayve's GAIA for autonomous cars, Genie could be extremely powerful for training general agents. Given the blistering rate of progress since last year, I wouldn't be shocked to see the Genie team successfully address many of these limitations in future releases. Fingers crossed.