---
layout: post
title: "Notes on Genie 3"
subtitle: "Genie 3 nails long-horizon visual coherence, but there's still work to be done on steerable, causal interaction."
---

<style>
/* Post-local styling for video figures */
figure.video { margin: 1.25rem 0; max-width: 560px}
figure.video video { display: block; width: 100%; height: auto; border-radius: 6px; }
figure.video figcaption { margin-top: 0.5rem; font-size: 0.95rem; line-height: 1.4; color: #666; text-align: center; }
@media (prefers-color-scheme: dark) { figure.video figcaption { color: #9aa0a6; } }
</style>

Genie 3 was recently debuted by DeepMind, to well-deserved enthusiasm. In less than two years, the model has made [an incredible leap](https://sites.google.com/view/genie-2024/home) in capabilities. DeepMind has scaled from five-second blurry snippets of 2D platformers to immersive, minutes-long, high-res interactions in self-consistent worlds. The model even runs in real time (although DeepMind didn't disclose how much compute it takes.)

To show off the model, they released impressive demosâ€”dragon flying, jet skiing and even an eerie [world within a world](https://x.com/jkbr_ai/status/1953154961988305384). But it's worth remembering that demos like these are cherry-picked to highlight strengths and paper over limitations. I wanted to note down a few things I *didn't see* in the Genie 3 demos; not as a nitpick, but to flag open research directions in world modelling for embodied AI.

## Static Environments
Most environments shown in the videos are relatively static. That isn't to say there's no physics: we see water splashes, dust clouds, rigid-body physics and mirrored reflections. But Genie is likely trained on mostly games, and those are game physics. When the model strays away from game-like interactions things can seem artificialâ€”see the jetski below. That's corroborated by Tejas Kulkarni on Twitter; behind the curtains, physics simulation is still limited.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Special thanks to <a href="https://twitter.com/GoogleDeepMind?ref_src=twsrc%5Etfw">@GoogleDeepMind</a> for inviting me to try out Genie 3. I&#39;m excited to share my thoughts on this early research prototype and also some of my live recordings below:<br><br>I spent the whole day playing with the system and when it works, it is truly mind blowingðŸ¤¯. It isâ€¦ <a href="https://t.co/JPW5sPEeF5">pic.twitter.com/JPW5sPEeF5</a></p>&mdash; Tejas Kulkarni (@tejasdkulkarni) <a href="https://twitter.com/tejasdkulkarni/status/1952737669894574264?ref_src=twsrc%5Etfw">August 5, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

As long as games provide the majority of the pretraining corpus, it's plausible that physics will be a challenge; game devs have to prioritise interactivity, so much of a game world is static. Plus, there's plenty that game engines can't render in real time, like cloth-on-cloth interactions, feathers and fur (more on [fur rendering here](https://www.youtube.com/watch?v=9dr-tRQzij4)). The static feel is strengthened by a lack of other agents, unless user-triggered by a world event. That might be because the model struggles with coherence with too many dynamic entities in the environment, or it can't model multi-agent interactions.

<figure class="video">
  <video width="560" height="315" controls>
    <source src="/videos/jetski.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <figcaption>The collision at 0:25 transfers no momentum to the other boat, in contrast to realistic interactions at 0:05.</figcaption>
</figure>
<figure class="video">
  <video width="560" height="315" controls>
    <source src="/videos/leaves_static.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <figcaption>The agent doesn't collide with the foliage, even walking through it at 0:39.</figcaption>
</figure>

## Interaction
For the most part, the demos only demonstrate navigation. Agents traverse a world and reveal their surroundings, but they don't *do* much. That could be due to a limited action space; the agent has access to a single "Act" button that needs to work in all environments. But it's also unclear how to define a richer action space. There's an almost fractal complexity here - should the agent press "play piano", specify notes, or puppeteer each finger individually? Again, because of the games in the corpus, it's unsurprising that the space is limited to high-level motions (eg. "press F to pay respects"). And it's not obvious to me how you would generate lower-level action conditioning without real-world interactions, more along the lines of the work being done on [robotics foundation models](https://www.physicalintelligence.company/blog/pi0).

To be clear, this is a hard problem. Unbounded interactivity means virtually infinite world states that the model has to be able to handle. The bottleneck isn't just physics, it's also real world knowledge. One demo shows an agent in an industrial bakery. To simulate arbitrary interactions, the model would need to know everything from bread baking and operating industrial mixers to how fire propagates in a floury environment (CC Samuel Pepys). DeepMind's game-playing agent SIMA can walk to a hose, but can't take it off the wall and water the garden. It'll be interesting to see whether future Genie models are capable of modelling deep interaction via self-supervised learning, or whether they will need to interact and learn in the real world.


<figure class="video">
  <video width="560" height="315" controls>
    <source src="/videos/hose.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <figcaption>The agent navigates to a hose as instructed, but that's all it can do.</figcaption>
</figure>

## Action consistency & Steerability
At different steps in a Genie 3 world, actions can be subtly different. And sometimes they aren't required at all. Take these examples:

<figure class="video">
  <video width="560" height="315" controls>
    <source src="/videos/critter.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <figcaption>At 0:22 the characters jumps without the agent pressing a button.</figcaption>
</figure>

<figure class="video">
  <video width="560" height="315" controls>
    <source src="/videos/cat_reverse.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <figcaption>Early in the video, â†‘ turns the bicycle and moves forwards relative to the camera. At the end it moves backwards, but doesn't rotate any more. Initially, the cat follows the path without action inputs.</figcaption>
</figure>

<figure class="video">
  <video width="560" height="315" controls>
    <source src="/videos/vaporetto.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <figcaption>Hitting â†‘ has no effect on the boat's forward motion.</figcaption>
</figure>


I think the cause is a conflict between visual consistency and steerability. The critter jumping over a gap is likely consistent with footage in the training data, as most players successfully jump the gap. By forcing a jump, Genie keeps the videogen safely in distribution. But railroading the agent impedes steerability. If the agent doesn't jump, the critter should fall. A lack of fidelity to actions could be problematic for agent learning within world models; if the world adjusts itself for mistakes, agents have no need to avoid them. That might be ok in training, but useful agents will need to transfer to the less-forgiving real world.

## Conclusion
To reiterate, the model is extremely impressive. With Genie, DeepMind has energised research into videogen world models (see [Matrix-GAME](https://x.com/Skywork_ai/status/1955237399912648842) and [OASIS](https://oasis-model.github.io/)). The models reflect a natural evolution of the Open-Endedness research agenda, developing truly general agents which can continually to learn and interact in an environment. Given the blistering rate of progress, I wouldn't be shocked to see these limitations solved in future releases. Fingers crossed.
