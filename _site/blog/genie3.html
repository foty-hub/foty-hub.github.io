<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Genie 3 - </title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css">
  <style>
    body { font-family: Arial, sans-serif; margin: 2rem; }
    header { margin-bottom: 2rem; }
    .tab { overflow: hidden; border-bottom: 1px solid #ccc; }
    .tablinks { background-color: inherit; border: none; outline: none; cursor: pointer; padding: 0.5rem 1rem; transition: 0.3s; display: inline-block; text-decoration: none; color: inherit; }
    .tablinks:hover { background-color: #ddd; }
    .tablinks.active { background-color: #ccc; }
    .tabcontent { display: none; padding: 1rem 0; }
    main { max-width: 820px;}
    article > .subtitle { margin-top: -0.5rem; margin-bottom: 1rem; color: #666; font-size: 1.1rem; line-height: 1.4; }
    @media (prefers-color-scheme: dark) { article > .subtitle { color: #9aa0a6; } }
    /* Research section layout */
    .research-list { display: grid; gap: 1.25rem; }
    .research-item { display: flex; align-items: flex-start; gap: 1rem; }
    .research-media { flex: 0 0 160px; width: 160px; height: 160px; border-radius: 8px; overflow: hidden; background: #e9ecef; }
    .research-media img, .research-media video { width: 100%; height: 100%; object-fit: cover; display: block; }
    .research-content h3 { margin: 0 0 0.25rem; font-size: 1.1rem; }
    .research-content p { margin: 0; }
    @media (max-width: 640px) {
      .research-item { flex-direction: column; }
      .research-media { width: 100%; height: auto; aspect-ratio: 1 / 1; }
    }
    /* Section spacing - a few line breaks */
    main section { margin-top: 5em; }
    main section:first-of-type { margin-top: 0; }
    @media (max-width: 640px) {
      main section { margin-top: 5em; }
    }
  </style>
  
  
</head>
<body>
  <header>
    <h1><a href="/"></a></h1>
    <nav class="tab" aria-label="Primary">
  <a class="tablinks" href="/">Home</a>
  <a class="tablinks" href="/blog">Blog</a>
</nav>

  </header>
  <main>
    <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Genie 3 - </title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css">
  <style>
    body { font-family: Arial, sans-serif; margin: 2rem; }
    header { margin-bottom: 2rem; }
    .tab { overflow: hidden; border-bottom: 1px solid #ccc; }
    .tablinks { background-color: inherit; border: none; outline: none; cursor: pointer; padding: 0.5rem 1rem; transition: 0.3s; display: inline-block; text-decoration: none; color: inherit; }
    .tablinks:hover { background-color: #ddd; }
    .tablinks.active { background-color: #ccc; }
    .tabcontent { display: none; padding: 1rem 0; }
    main { max-width: 820px;}
    article > .subtitle { margin-top: -0.5rem; margin-bottom: 1rem; color: #666; font-size: 1.1rem; line-height: 1.4; }
    @media (prefers-color-scheme: dark) { article > .subtitle { color: #9aa0a6; } }
  </style>
</head>
<article>
  <h1>Genie 3</h1>
  
  <p class="subtitle">Deepmind's new  model is mindblowing, but the videos hint at remaining challenges.</p>
  
  <div>
    <style>
/* Post-local styling for video figures */
figure.video { margin: 1.25rem 0; max-width: 560px}
figure.video video { display: block; width: 100%; height: auto; border-radius: 6px; }
figure.video figcaption { margin-top: 0.5rem; font-size: 0.95rem; line-height: 1.4; color: #666; text-align: center; }
@media (prefers-color-scheme: dark) { figure.video figcaption { color: #9aa0a6; } }
</style>

<p>DeepMind recently debuted Genie 3, a video-generation world model, to well-deserved enthusiasm. In less than two years, the model has made <a href="https://sites.google.com/view/genie-2024/home">an incredible leap</a> in capabilities. The Genie team has scaled from five-second blurry snippets of 2D platformers to immersive, minute-long, high-res interactions in self-consistent worlds. The model even runs in real time (no system requirements though.)</p>

<p>To show off the model, they released impressive demos‚Äîdragon flying, jet skiing and even an eerie <a href="https://x.com/jkbr_ai/status/1953154961988305384">world within a world</a>. But demos like these are cherry-picked to highlight a model‚Äôs strengths. I wanted to note a few things I <em>didn‚Äôt see</em> in the Genie 3 demos; not as a nitpick, but to flag open research directions in world modelling for embodied AI.</p>

<h2 id="dynamic-environments">Dynamic Environments</h2>
<p>Most environments shown in the videos are relatively static. That isn‚Äôt to say there‚Äôs no physics: we see water splashes, dust clouds, rigid-body physics and mirrored reflections. But Genie is likely trained on mostly games, and those are game physics. When the model strays away from game-like interactions things can seem artificial‚Äîsee the jetski below. That‚Äôs corroborated by Tejas Kulkarni on Twitter; behind the curtains, physics simulation is still limited.</p>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Special thanks to <a href="https://twitter.com/GoogleDeepMind?ref_src=twsrc%5Etfw">@GoogleDeepMind</a> for inviting me to try out Genie 3. I&#39;m excited to share my thoughts on this early research prototype and also some of my live recordings below:<br /><br />I spent the whole day playing with the system and when it works, it is truly mind blowingü§Ø. It is‚Ä¶ <a href="https://t.co/JPW5sPEeF5">pic.twitter.com/JPW5sPEeF5</a></p>&mdash; Tejas Kulkarni (@tejasdkulkarni) <a href="https://twitter.com/tejasdkulkarni/status/1952737669894574264?ref_src=twsrc%5Etfw">August 5, 2025</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>As long as games provide the majority of the pretraining corpus, it‚Äôs plausible that physics will be a challenge; each new interaction takes work, so devs naturally leave most of a game world static. Plus, there‚Äôs plenty that game engines can‚Äôt render in real time, like cloth-on-cloth interactions, feathers and fur (more on <a href="https://www.youtube.com/watch?v=9dr-tRQzij4">fur rendering here</a>). The static feel is reinforced by a lack of other agents interacting with the environment (unless user-triggered by a world event). It‚Äôs possible that the model struggles with coherence with too many dynamic entities in the environment, or can‚Äôt model multi-agent interactions.</p>

<figure class="video">
  <video width="560" height="315" controls="">
    <source src="/assets/videos/jetski.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>The collision at 0:25 transfers no momentum to the other boat, in contrast to realistic interactions at 0:05.</figcaption>
</figure>
<figure class="video">
  <video width="560" height="315" controls="">
    <source src="/assets/videos/leaves_static.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>The agent doesn't collide with the foliage, even walking through it at 0:39.</figcaption>
</figure>

<h2 id="interaction">Interaction</h2>
<p>For the most part, the demos only demonstrate navigation. Agents traverse a world and reveal their surroundings, but they don‚Äôt <em>do</em> much. That could be due to a limited action space; the agent has access to a single ‚ÄúAct‚Äù button that needs to work in all environments. But it‚Äôs also unclear how to define a richer action space. There‚Äôs an almost fractal complexity here - should the agent press ‚Äúplay piano‚Äù, specify notes, or puppeteer each finger individually? Again, because of the games in the corpus, it‚Äôs unsurprising that the space is limited to high-level motions (eg. ‚Äúpress F to pay respects‚Äù). And it‚Äôs not obvious to me how you would generate lower-level action conditioning without real-world interactions, more along the lines of the work being done on <a href="https://www.physicalintelligence.company/blog/pi0">robotics foundation models</a>.</p>

<p>To be clear, this is a hard problem. Unbounded interactivity means virtually infinite world states that the model has to be able to handle. The bottleneck isn‚Äôt just physics, it‚Äôs also real world knowledge. One demo shows an agent in an industrial bakery. To simulate arbitrary interactions, the model would need to know everything from bread baking and operating industrial mixers to how fire propagates in a floury environment (CC Samuel Pepys). DeepMind‚Äôs game-playing agent SIMA can walk to a hose, but can‚Äôt take it off the wall and water the garden. It‚Äôll be interesting to see whether future Genie models are capable of modelling deep interaction via self-supervised learning, or whether real world interaction is a necessary component of the training process.</p>

<figure class="video">
  <video width="560" height="315" controls="">
    <source src="/assets/videos/hose.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>The agent navigates to a hose as instructed, but that's all it can do.</figcaption>
</figure>

<h2 id="action-consistency-and-steerability">Action Consistency and Steerability</h2>
<p>At different steps in a Genie 3 world, actions can be subtly different. And sometimes they aren‚Äôt required at all. Take these examples:</p>

<figure class="video">
  <video width="560" height="315" controls="">
    <source src="/assets/videos/critter.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>At 0:22 the characters jumps without the agent pressing a button.</figcaption>
</figure>

<figure class="video">
  <video width="560" height="315" controls="">
    <source src="/assets/videos/cat_reverse.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Initially, the cat follows the path without action inputs. ‚Üë initially faces the cat forwards, but later directs it backwards. The jump button only works inconsistently.</figcaption>
</figure>

<figure class="video">
  <video width="560" height="315" controls="">
    <source src="/assets/videos/vaporetto.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption>Hitting ‚Üë has no effect on the boat's forward motion.</figcaption>
</figure>

<p>I think the cause is a conflict between visual consistency and steerability. The critter jumping over a gap is likely consistent with footage in the training data, as most players successfully jump the gap. By forcing a jump, Genie keeps the videogen safely in distribution. But railroading the agent impedes steerability. If the agent doesn‚Äôt jump, the critter should fall. A lack of fidelity to actions could be problematic for agent learning within world models; if the world adjusts itself for mistakes, agents have no need to avoid them. That might be ok in training, but useful agents will need to transfer to the less-forgiving real world.</p>

<h2 id="conclusion">Conclusion</h2>
<p>To reiterate, the model is extremely impressive. With Genie, DeepMind has energised research into videogen world models (see <a href="https://x.com/Skywork_ai/status/1955237399912648842">Matrix-GAME</a> and <a href="https://oasis-model.github.io/">OASIS</a>). The models reflect a natural evolution of the Open-Endedness research agenda; developing truly general agents which can continually interact and learn in an environment. Given the blistering rate of progress, I wouldn‚Äôt be shocked to see these limitations solved in future releases. Fingers crossed.</p>

  </div>
  <p>Posted on August 15, 2025</p>
</article>

  </main>
  <footer>
    <p>&copy; 2025 Alex Inch</p>
  </footer>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      var tabContents = document.querySelectorAll('.tabcontent');
      var navLinks = Array.prototype.slice.call(document.querySelectorAll('nav.tab .tablinks'));

      function setActiveLink(predicate) {
        navLinks.forEach(function(l){ l.classList.remove('active'); });
        var active = navLinks.find(predicate);
        if (active) active.classList.add('active');
      }

      // If this page has tab content (home), wire up in-page tabs
      if (tabContents.length > 0) {
        function showTab(id) {
          tabContents.forEach(function(c){ c.style.display = 'none'; });
          var el = document.getElementById(id);
          if (el) el.style.display = 'block';
          setActiveLink(function(l){ return (l.getAttribute('href') || '').endsWith('#' + id); });
        }

        // Click handlers on in-page links
        navLinks.forEach(function(link){
          var href = link.getAttribute('href') || '';
          if (href.indexOf('#') !== -1) {
            link.addEventListener('click', function(e){
              var hash = (link.hash || '').replace('#','');
              if (hash) {
                e.preventDefault();
                history.replaceState(null, '', '#' + hash);
                showTab(hash);
              }
            });
          }
        });

        // Initial tab: from hash or default to About
        var initial = (location.hash || '#About').replace('#','');
        showTab(initial);
      } else {
        // No tab contents present (e.g., blog pages): set active nav by path
        var path = location.pathname;
        if (path.indexOf('/blog') === 0) {
          setActiveLink(function(l){ return (l.getAttribute('href') || '').indexOf('/blog') !== -1; });
        } else {
          setActiveLink(function(l){ return (l.getAttribute('href') || '').replace(location.origin, '') === '/'; });
        }
      }
    });
  </script>
</body>
</html>
